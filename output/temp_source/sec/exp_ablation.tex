\subsection{Visual Recognition Capability Validation}

To verify whether limited performance on temporal reasoning tasks stems from visual encoding deficiencies in models, we evaluated the visual recognition capabilities of five open-source models on the UCM (UC Merced Land Use) remote sensing image classification dataset. Table~\ref{tab:ucm} presents the accuracy of all models on this standard visual classification task.

The UCM dataset contains 21 categories, and for each sample we select 5 categories including the correct category, with a random baseline of approximately 20\%. Experimental results demonstrate that all evaluated models exhibit exceptional visual recognition capabilities, with accuracies all above 0.915, with InternVL and MiMo-VL achieving the highest accuracy of 0.955, Ovis reaching 0.94, MiniCPM reaching 0.93, and Qwen3-VL reaching 0.915. These results significantly exceed random guessing levels, demonstrating that all models possess robust remote sensing image feature extraction and category recognition capabilities.

This clearly excludes visual encoding deficiencies as the primary cause of limited temporal reasoning performance. The high accuracy of models on UCM (average 93.9\%) contrasts sharply with their near-random baseline performance on TIU tasks (such as ICE and EII), indicating that temporal reasoning difficulties do not stem from models' inability to recognize or understand visual content in individual images, but rather from models' difficulty in establishing temporal relationships across multiple images and quantifying temporal intervals.

\subsection{Cross-Domain Transferability Analysis}

To evaluate whether temporal understanding capabilities learned on TimePerceptBench possess generalizability, we conducted cross-domain validation experiments on the TemporalVQA benchmark. TemporalVQA is a non-remote-sensing temporal reasoning benchmark containing image sequences of everyday life scenes, with time spans (with minimum units of seconds to minutes) significantly different from our remote sensing dataset (day to year levels). Table~\ref{tab:crossdomain} presents the performance of five open-source models on TemporalVQA before and after fine-tuning, including evaluations across TOU (720 samples) and TIU (125 samples) dimensions.

Experimental results reveal interesting patterns in temporal reasoning capability transfer. Overall, models fine-tuned on TimePerceptBench demonstrate certain degrees of performance improvement on TemporalVQA, demonstrating that learned temporal understanding capabilities possess certain cross-domain generalizability. However, a more significant finding is that TIU capability transfer effects are notably superior to TOU capability transfer. TIU tasks improve by approximately 8.0 percentage points on average after fine-tuning (from baseline 25.6\%--63.2\% to 50.4\%--64.8\%), while TOU tasks improve by only approximately 3.9 percentage points (from baseline 50.0\%--67.5\% to 58.6\%--67.5\%). This difference is also confirmed at the individual model level.

This asymmetry in transfer patterns reveals essential differences between different dimensions of temporal reasoning. The stronger cross-domain transfer of TIU capabilities suggests that cognitive mechanisms underlying temporal interval understanding may be more universal and abstract, not strongly constrained by specific image domains or temporal scales. Knowledge learned by models about relationships between degrees of change and temporal spans can effectively transfer across different visual domains and temporal scales. In contrast, limited TIU transfer may reflect stronger dependence of temporal order understanding on domain-specific patterns---specific temporal patterns learned in remote sensing images, such as seasonal changes and urban development, are difficult to directly apply to different temporal logic in everyday scenes. Additionally, Task 1 (temporal order understanding) improves from baseline 46.39\%--65.92\% to 57.4\%--66.39\% after fine-tuning, with improvement margins between 10--20 percentage points, exhibiting transfer patterns similar to main TOU tasks. Notably, different models exhibit different characteristics in cross-domain transfer: InternVL achieves the largest transfer gains on TIU, while Qwen3-VL's TIU performance slightly decreases after fine-tuning (from 63.2\% to 58.4\%), potentially reflecting overfitting or model architecture preferences for specific temporal scales.

These cross-domain experimental results have important theoretical and practical implications. They demonstrate that temporal understanding capabilities learned on TimePerceptBench are not simple dataset overfitting, but possess certain degrees of generalization capability. Second, the stronger transferability of TIU compared to TOU, combined with its lower absolute performance on TimePerceptBench, suggests that TIU tasks, while more challenging, may possess greater room for improvement and stronger universal value.
