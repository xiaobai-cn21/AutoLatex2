\section{Conclusion}

We present TimePerceptBench, a comprehensive benchmark for evaluating temporal understanding capabilities in multimodal large language models. By leveraging remote sensing imagery with objective timestamps, we systematically decompose temporal understanding into two complementary core capabilities: Temporal Order Understanding (TOU) and Temporal Interval Understanding (TIU), designing nine fine-grained subtasks to evaluate model performance across different granularities and difficulty levels.

We conducted comprehensive evaluation of six advanced multimodal large language models, with experimental results revealing significant limitations in current models' temporal reasoning capabilities. Unfine-tuned models perform near random baselines on temporal sequence reordering and temporal interval quantification tasks, indicating that temporal understanding capabilities are not inherent to these models. Through 93.9\% high accuracy on the UCM dataset, we clearly exclude visual encoding deficiencies as the primary cause of limited performance, confirming the inherent challenges of temporal reasoning itself.

Fine-tuning experiments demonstrate that models can significantly improve temporal reasoning capabilities after targeted training, but TOU and TIU tasks exhibit different learning characteristics. TOU tasks show more pronounced improvements, with reordering tasks achieving PNR up to 4.35, while TIU tasks, especially absolute temporal quantification, remain highly challenging, indicating fundamental difficulties in current model architectures for quantitative temporal reasoning. Cross-domain validation experiments further reveal interesting transfer patterns: TIU capabilities demonstrate stronger cross-domain transferability compared to TOU (+8.0\% vs +3.9\%), suggesting that despite TIU tasks being more challenging, the cognitive mechanisms they rely upon may be more universal with greater potential for improvement.

TimePerceptBench provides a systematic evaluation framework and in-depth analytical insights for temporal understanding research. We hope this benchmark will advance further development of temporal reasoning capabilities in multimodal large language models and facilitate the construction of more intelligent and comprehensive visual understanding systems.
