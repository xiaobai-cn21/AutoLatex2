\section{Related Works}

\subsection{Temporal Reasoning Benchmarks for Large Language Models}

In recent years, the evaluation of temporal reasoning in language models has undergone significant development, with researchers developing increasingly sophisticated benchmarks to assess different aspects of temporal understanding. TimeQA \citep{Chen21} introduced time-sensitive question answering tasks focusing on temporal knowledge probing, demonstrating difficulties in time-sensitive information retrieval by models. TEMPLAMA \citep{Dhingra22} specifically targeted temporal knowledge by probing language models' understanding of temporal facts, revealing significant gaps in temporal knowledge representation. TEMPREASON \citep{Tan23} advanced the field by evaluating complex temporal relationship understanding, showing that even advanced models struggle with implicit temporal reasoning tasks.

Building upon these foundations, more comprehensive benchmarks have emerged. TRAM proposed the most extensive evaluation to date, containing 526.7k questions spanning ten different temporal reasoning tasks covering order, arithmetic, frequency, and duration aspects. Their systematic evaluation revealed that even the best-performing models, including GPT-4, fall significantly short of human-level performance, with GPT-4 \citep{Achiam23} achieving 84.4\% accuracy while human performance exceeds 94\%. TimeBench introduced a hierarchical framework categorizing temporal reasoning into symbolic, commonsense, and event temporal reasoning across 16 subtasks, demonstrating substantial performance gaps between state-of-the-art large language models and human capabilities. Their analysis revealed that models particularly struggle with computational, conversion, and comparison errors in temporal expressions, with multi-step reasoning remaining a significant challenge. Test of Time \citep{Fatemi24} specifically evaluates semantic understanding and arithmetic computation capabilities in temporal reasoning through synthetic datasets, avoiding issues with models leveraging prior knowledge, while PAT-Questions \citep{Meem24} focuses on present-anchored temporal question answering, addressing the dynamic challenges of answers changing over time and providing an automatically updated evaluation framework.

\subsection{Multimodal Temporal Understanding}

Despite significant progress in text-based temporal reasoning, multimodal temporal understanding remains a significantly underexplored domain. Most existing multimodal benchmarks focus on static image understanding or simple video tasks that fail to adequately evaluate temporal reasoning capabilities across extended sequences \citep{Cai24}. The transition from textual to visual temporal reasoning introduces unique challenges, as models must extract temporal relationships from visual changes rather than explicit linguistic cues, requiring fundamentally different cognitive processes.

Recent pioneering efforts have begun addressing this critical gap through specialized multimodal benchmarks. TOMATO \citep{Shangguan24} evaluates multimodal large language models' video understanding through comprehensive multi-frame analysis and frame order sensitivity assessment, focusing on sequential video frame processing and temporal consistency. Their evaluation revealed significant difficulties in temporal sequence understanding among current multimodal models, particularly in maintaining consistency across longer video sequences. TVBench \citep{Cores24} advances video-language evaluation by systematically addressing bias and static cue reliance in video-language tasks, advocating for more rigorous temporal evaluation protocols capable of distinguishing genuine temporal understanding from superficial pattern recognition.

Most directly relevant to our work, TemporalVQA introduced temporal order understanding and time-lapse estimation tasks using image pairs, representing the first systematic attempt to evaluate temporal reasoning in multimodal language models using static image sequences. Their comprehensive evaluation revealed striking limitations in current multimodal large language models' temporal reasoning capabilities, with advanced models like GPT-4V achieving only 49.1\% accuracy on temporal order tasks and even lower performance on time-lapse estimation. This work demonstrated that the transition from text-based temporal reasoning to visual temporal reasoning represents a fundamental challenge for current multimodal architectures, highlighting the urgent need for more comprehensive evaluation frameworks capable of systematically assessing temporal understanding across diverse scenarios and extended temporal sequences.

Current temporal reasoning benchmarks exhibit several key limitations that constrain their effectiveness in comprehensively evaluating temporal understanding capabilities. Most existing evaluations primarily focus on short-term temporal relationships or simple sequential patterns, failing to capture the complexity of extended temporal reasoning required in real-world applications involving long-term planning and understanding. The reliance of many benchmarks on subjective temporal annotations or synthetic data limits evaluation objectivity and ecological validity, potentially introducing biases that affect assessment reliability and generalization to practical scenarios.
