\subsection{Experimental Setup}

We conducted comprehensive experiments on TimePerceptBench to systematically evaluate the temporal understanding capabilities of current state-of-the-art multimodal large language models. Our evaluation encompasses six representative models selected for their exceptional visual processing capabilities: GPT-4o mini and five advanced open-source models, including InternVL3.5-8B-Instruct, Ovis2.5-9B, Qwen3-VL-8B-Instruct, MiniCPM-V-4.5, and MiMo-VL-7B-RL.

During data preprocessing, we uniformly resized all images to $256\times256$ resolution to ensure compatibility across different model architectures while preserving the critical visual information necessary for temporal reasoning. Each subtask of TOU and TIU consists of 800 training samples and 200 test samples. For fine-tuning experiments, we adopted LoRA (Low-Rank Adaptation) as a parameter-efficient fine-tuning strategy with carefully tuned hyperparameters: rank $r=8$, scaling factor $\alpha=16$, and dropout rate of 0.1. The training process employed a learning rate of $5\times10^{-5}$ with a cosine annealing scheduler, batch size of 2, and gradient accumulation steps of 8. Models were fine-tuned for three epochs on the designated training set. Due to API limitations, GPT-4o mini was evaluated only in its baseline configuration without fine-tuning.

We evaluated models under two primary experimental conditions: (1) \textbf{Baseline:} original pretrained models without additional fine-tuning, and (2) \textbf{Fine-tuned:} models fine-tuned on our temporal reasoning tasks using a comprehensive temporal task training set composed of all subtask training sets, ensuring models do not exhibit particular bias toward any specific task and maintaining balanced temporal understanding.

For reordering tasks, we used the aforementioned PNR as the primary evaluation metric, while other selection or judgment tasks employed accuracy as the metric.

To validate the generalizability and transferability of learned temporal reasoning capabilities, we conducted additional cross-domain experiments on TemporalVQA, a non-remote-sensing benchmark (with image spans significantly different from our remote sensing datasets) that includes two tasks: image pair order judgment and image pair span prediction. This validation enables us to assess whether improvements observed on TimePerceptBench reflect genuine temporal understanding capabilities rather than dataset-specific overfitting.
